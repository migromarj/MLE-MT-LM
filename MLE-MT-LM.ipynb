{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tcame\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bardapi import BardCookies\n",
    "from sydney import SydneyClient\n",
    "from dotenv import load_dotenv\n",
    "from contexto.comparacion import Similitud, Distancia, DiferenciaStrings\n",
    "from contexto.vectorizacion import *\n",
    "import markdown\n",
    "import re\n",
    "import random\n",
    "import string\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('summarize', 'facebook/bart-large-cnn'),\n",
    "    ('summarize', 'google/pegasus-large'),\n",
    "    ('summarize', 'marianna13/flan-t5-base-summarization'),\n",
    "    ('toxic', 's-nlp/roberta_toxicity_classifier'),\n",
    "    ('toxic', 'citizenlab/distilbert-base-multilingual-cased-toxicity', 'inputs'),\n",
    "    ('toxic', 'martin-ha/toxic-comment-model'),\n",
    "    ('spam', 'rafacost/bert_base_pt_en_cased_email_spam'),\n",
    "    ('spam', 'h-e-l-l-o/email-spam-classification-merged'),\n",
    "    ('spam', 'dima806/email-spam-detection-roberta'),\n",
    "    ('translate', 't5-base'),\n",
    "    ('translate', 'allenai/wmt16-en-de-12-1'),\n",
    "    ('translate', 'facebook/wmt19-en-de'),\n",
    "    ('fillmask', 'bert-base-uncased', '[MASK]'),\n",
    "    ('fillmask', 'vinai/bertweet-base', '<mask>'),\n",
    "    ('fillmask', 'roberta-base', '<mask>'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['binario', 'no binario', 'transgénero', 'cisgénero', 'agénero']\n",
    "ages = list(range(18, 100)) \n",
    "races = ['caucásica', 'afrodescendiente', 'asiática', 'latina', 'indígena']\n",
    "orientations = ['heterosexual', 'homosexual', 'bisexual', 'asexual', 'pansexual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_model(model, input):\n",
    "\n",
    "    BASE_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGING_FACE_API_KEY')}\"}\n",
    "    new_input = input\n",
    "    parameters = {}\n",
    "\n",
    "    if model[0] == 'fillmask':\n",
    "        new_input = input.replace('<mask>', model[2])\n",
    "    elif model[0] == 'translate':\n",
    "        parameters = {\"src_lang\": \"en_XX\", \"tgt_lang\": \"tgt_XX\"}\n",
    "\n",
    "    query = {\"inputs\": new_input, \"wait_for_model\": True, \"parameters\":parameters}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(BASE_URL + model[1], json=query, headers=headers)\n",
    "    except Exception as exception:\n",
    "        print(exception)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_plain_text(input_text):\n",
    "    input_text = re.sub(r'\\[\\^\\d+\\^\\]', '', input_text)\n",
    "    plain_text = markdown.markdown(input_text, output_format='html')\n",
    "    plain_text = re.sub(r'<[^>]*>\\s*', '', plain_text)\n",
    "    return plain_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_answer_bing(input):\n",
    "    indice_inicio = input.find(\":\") \n",
    "    \n",
    "    if indice_inicio != -1:\n",
    "        indice_inicio += 1 \n",
    "\n",
    "        return input[indice_inicio:].strip()\n",
    "    else:\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentaje(input):\n",
    "    number = re.findall(r'\\d+%', input)\n",
    "    return int(number[0].replace('%', '')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_demografic_context():\n",
    "    \n",
    "    genre = random.choice(genres)\n",
    "    age = random.choice(ages)\n",
    "    race = random.choice(races)\n",
    "    orientation = random.choice(orientations)\n",
    "\n",
    "    \n",
    "    frase = f\"The following question has been written by a person of gender {genre}, of {age} years, with orientation {orientation} and of race {race}\"\n",
    "\n",
    "    return frase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def request_to_bing(question, question2 = None, type=\"q&a\", ):\n",
    "\n",
    "    prompt = \"\"\n",
    "    if (type == \"q&a\"):\n",
    "        prompt = \"Answer me the following question in plain text without using quotes: \"\n",
    "    elif (type == \"change_order\"):\n",
    "        prompt = \"Change the order of the next sentence: \"\n",
    "    elif (type == \"add_random_words\"):\n",
    "        prompt = \"Add random words, do not replace, to the next sentence but don't let the phrase lose its meaning: \"\n",
    "    elif (type == \"remplace_named_entities\"):\n",
    "        prompt = \"Replace all named entities to alter the demographic context of the following sentence, give me only the new sentence: \"\n",
    "    elif (type == \"use_negation\"):\n",
    "        prompt = \"Modify the following sentence using negation, give me only the new sentence: \"\n",
    "    elif (type == \"introducce_demografic_context\"):\n",
    "        demografic_context = generate_demografic_context()\n",
    "        prompt = prompt + demografic_context + \"\\n\"\n",
    "    elif (type == \"toxic\"):\n",
    "        prompt = \"Is the following sentence toxic? Say me only yes or no, i dont want anything else \" + \"\\n\"\n",
    "    elif (type == \"spam\"):\n",
    "        prompt = \"Is the following sentence spam? Say me only yes or no, i dont want anything else \" + \"\\n\"\n",
    "    elif (type == \"compare\"):\n",
    "        prompt = \"Compare the following sentences.Say me only the % number of similarity, i dont want anyting else: \" + \"\\n\"\n",
    "\n",
    "    async with SydneyClient(style=\"precise\") as sydney:\n",
    "        if (type == \"compare\"):\n",
    "            response = await sydney.ask(prompt + question + \"\\n\" + question2, citations=False)\n",
    "        else:\n",
    "            response = await sydney.ask(prompt + question, citations=False)\n",
    "        response = convert_to_plain_text(response)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_to_bard(question, question2 = None, type=\"q&a\"):\n",
    "    \n",
    "    cookie_dict = {\n",
    "        \"__Secure-1PSID\": os.getenv('SECURE_1PSID'),\n",
    "        \"__Secure-1PSIDTS\": os.getenv('SECURE_1PSIDTS'),\n",
    "        \"__Secure-1PSIDCC\": os.getenv('SECURE_1PSIDCC'),\n",
    "    }\n",
    "    \n",
    "    prompt = \"\"\n",
    "    if (type == \"q&a\"):\n",
    "        prompt = \"Answer me the following question in plain text without using quotes: \"\n",
    "    elif (type == \"toxic\"):\n",
    "        prompt = \"Is the following sentence toxic? Say me only yes or no, i dont want anything else \" + \"\\n\"\n",
    "    elif (type == \"spam\"):\n",
    "        prompt = \"Is the following sentence spam? Say me yes or no, i dont want anything else \" + \"\\n\"\n",
    "    elif (type == \"compare\"):\n",
    "        prompt = \"Compare the following sentences and give me only the % number of similarity, i dont want anyting else: \" + \"\\n\"\n",
    "\n",
    "    bard = BardCookies(cookie_dict=cookie_dict)\n",
    "    if (type == \"compare\"):\n",
    "        response = bard.get_answer(prompt + question + \"\\n\" + question2)['content']\n",
    "    else:\n",
    "        response = bard.get_answer(prompt + question)['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madrid.\n"
     ]
    }
   ],
   "source": [
    "print(request_to_bard(\"What is the capital of Spain?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERTURBACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_characters(input, nivel):\n",
    "    if nivel < 1 or nivel > 10:\n",
    "       return \"Level must be between 1 and 10.\"\n",
    "    \n",
    "    caracteres = list(input)\n",
    "    indices_a_sustituir = [i for i in range(len(caracteres)) if caracteres[i] in string.ascii_letters]\n",
    "    num_caracteres_a_sustituir = int(len(indices_a_sustituir) * nivel / 20)\n",
    "    indices_a_sustituir = random.sample(indices_a_sustituir, num_caracteres_a_sustituir)\n",
    "    \n",
    "    for indice in indices_a_sustituir:\n",
    "        caracteres[indice] = random.choice(string.ascii_letters)\n",
    "    \n",
    "    return ''.join(caracteres)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bello hpw are Jou? It's beBb a whiDe XiSce you anV I Met.\n"
     ]
    }
   ],
   "source": [
    "print(replace_characters(\"Hello how are you? It's been a while since you and I met.\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_characters(input, level):\n",
    "    if level < 1 or level > 10:\n",
    "        return \"Level must be between 1 and 10.\"\n",
    "    \n",
    "    characters = list(input)\n",
    "    num_characters_to_add = int(len(characters) * level / 20)\n",
    "    \n",
    "    for _ in range(num_characters_to_add):\n",
    "        index = random.randint(0, len(characters))\n",
    "        characters.insert(index, random.choice(string.ascii_letters))\n",
    "    \n",
    "    return ''.join(characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HKeello how are you? Itu's zbCeend a while sinXceV you mCand I metF.\n"
     ]
    }
   ],
   "source": [
    "print(add_characters(\"Hello how are you? It's been a while since you and I met.\", 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def add_random_words(input):\n",
    "    response = await request_to_bing(input, \"add_random_words\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you doing today? It's been quite a while since the last time you and I had the chance to meet.\n"
     ]
    }
   ],
   "source": [
    "text_random_words = await add_random_words(\"Hello how are you? It's been a while since you and I met.\")\n",
    "print(text_random_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def remplace_named_entities(input):\n",
    "    response = await request_to_bing(input, \"remplace_named_entities\")\n",
    "    return get_only_answer_bing(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akio and Yumi are from Japan. They are 30 years old. They are married. They have three children.\n"
     ]
    }
   ],
   "source": [
    "text_remplace_named_entities = await remplace_named_entities(\"Tomas and Maria are from Spain. They are 20 years old. They are married. They have two children.\")\n",
    "print(text_remplace_named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def change_order(input):\n",
    "    response = await request_to_bing(input, \"change_order\")\n",
    "    return get_only_answer_bing(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"It's been a while since you and I met. Hello, how are you?\"\n"
     ]
    }
   ],
   "source": [
    "text_change_order = await change_order(\"Hello how are you? It's been a while since you and I met.\")\n",
    "print(text_change_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def use_negation(input):\n",
    "    response = await request_to_bing(input, \"use_negation\")\n",
    "    return get_only_answer_bing(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? It hasn't been long since you and I met. I don't like to eat pizza.\n"
     ]
    }
   ],
   "source": [
    "text_use_negation = await use_negation(\"Hello how are you? It's been a while since you and I met. I like to eat pizza.\")\n",
    "print(text_use_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def introducce_demografic_context(input):\n",
    "    response = await request_to_bing(input, \"introducce_demografic_context\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is Madrid.\n"
     ]
    }
   ],
   "source": [
    "text_demografic_context = await introducce_demografic_context(\"What is the capital of Spain?\")\n",
    "print(text_demografic_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLANTILLAS RELACIONES METAMÓRFICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_equivalence_mrt(model,original_imput, perturbed_input):\n",
    "    \n",
    "    if (model[0] != 'toxic' and model[0] != 'spam'):\n",
    "        return \"This model is not supported for this test\"\n",
    "     \n",
    "    original_output = request_to_model(model, original_imput)[0][0]['label']\n",
    "    perturbed_output = request_to_model(model, perturbed_input)[0][0]['label']        \n",
    "    \n",
    "    return original_output == perturbed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "original_imput = \"You are an idiot\"\n",
    "disturbed_input = add_characters(\"You are an idiot\",2)\n",
    "res = input_equivalence_mrt(models[3], original_imput, disturbed_input)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def equivalence_set_mrt(model, input):\n",
    "    \n",
    "    original_output = request_to_model(model, input)[0][0]['label']    \n",
    "    llms_outputs = []\n",
    "\n",
    "    if (model[0] == 'toxic'):\n",
    "        \n",
    "        if original_output == \"neutral\":\n",
    "            original_output = \"no\"\n",
    "        elif original_output == \"non-toxic\":\n",
    "            original_output = \"no\"\n",
    "        elif original_output == \"toxic\":\n",
    "            original_output = \"yes\"\n",
    "        \n",
    "        bing_output = await request_to_bing(input, \"toxic\")\n",
    "        bard_output = request_to_bard(input, \"toxic\")\n",
    "        llms_outputs.append(bing_output.lower())\n",
    "        llms_outputs.append(bard_output.lower())\n",
    "        \n",
    "    elif (model[0] == 'spam'):\n",
    "        \n",
    "        if original_output == \"No spam\":\n",
    "            original_output = \"no\"\n",
    "        elif original_output == \"Ham\":\n",
    "            original_output = \"no\"\n",
    "        elif original_output == \"Spam\":\n",
    "            original_output = \"yes\"\n",
    "        \n",
    "        bing_output = await request_to_bing(input, \"spam\")\n",
    "        bard_output = request_to_bard(input, \"spam\")\n",
    "        llms_outputs.append(bing_output.lower())\n",
    "        llms_outputs.append(bard_output.lower())\n",
    "        \n",
    "    else: \n",
    "        return \"This model is not supported for this test\"\n",
    "\n",
    "    for llm_output in llms_outputs:\n",
    "        if original_output != llm_output:\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(await equivalence_set_mrt(models[8], \"Buy a car in my store and get a 20 discount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_distance_answers(input,input2,mode = 0):\n",
    "    if (mode == 0):\n",
    "        response_bing = await request_to_bing(input,input2,\"compare\")\n",
    "        response_bard = request_to_bard(input,input2, \"compare\")\n",
    "        percentaje_bing = get_percentaje(response_bing)\n",
    "        percentaje_bard = get_percentaje(response_bard)\n",
    "        mean = (percentaje_bing + percentaje_bard) / 2\n",
    "        return mean\n",
    "    else:\n",
    "        \n",
    "        # Similarity\n",
    "        v_bow =  VectorizadorFrecuencias()\n",
    "        v_tf =  VectorizadorFrecuencias(tipo='tfidf', idf=False)\n",
    "        v_tfidf = VectorizadorFrecuencias(tipo='tfidf')\n",
    "        v_hashing = VectorizadorHash()\n",
    "        test_texts = [input,input2]\n",
    "        v_bow.fit(test_texts)\n",
    "        v_tf.fit(test_texts)\n",
    "        v_tfidf.fit(test_texts)\n",
    "\n",
    "        vectors = {}\n",
    "        keys = ['bow', 'tf', 'tfidf', 'hash']\n",
    "        for i, v in enumerate([v_bow, v_tf, v_tfidf, v_hashing]):\n",
    "            vectors[keys[i]] = v.vectorizar(test_texts)\n",
    "\n",
    "        s_bow = Similitud(v_bow)\n",
    "        s_tf = Similitud(v_tf)\n",
    "        s_tfidf = Similitud(v_tfidf)\n",
    "        s_hashing = Similitud(v_hashing)\n",
    "\n",
    "        cosine_bow = s_bow.coseno(test_texts)\n",
    "        percentage_bow = cosine_bow[0][1] * 100\n",
    "        cosine_tf = s_tf.coseno(test_texts)\n",
    "        percentage_tf = cosine_tf[0][1] * 100\n",
    "        cosine_hashing = s_hashing.coseno(test_texts)\n",
    "        percentage_hashing = cosine_hashing[0][1] * 100\n",
    "        cosine_tfidf = s_tfidf.coseno(test_texts)\n",
    "        percentage_tfidf = cosine_tfidf[0][1] * 100\n",
    "\n",
    "        # Difference\n",
    "        d_hashing = Distancia(v_hashing)\n",
    "        hamming_hashing = 1- d_hashing.hamming(test_texts)[0][1]\n",
    "\n",
    "        # dictionary of \"Difference\"\n",
    "        difference = {'hash': hamming_hashing}\n",
    "        similarity = {'bow': percentage_bow, 'tf': percentage_tf, 'tfidf': percentage_tfidf, 'hash': percentage_hashing}\n",
    "        return difference, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.5\n"
     ]
    }
   ],
   "source": [
    "texto1 = \"Hola que tal\"\n",
    "texto2 = \"Hola que tal estas\"\n",
    "print(await get_distance_answers(texto1,texto2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def distance_set_mrt(original_input, model='q&a', mode=0, threshold=70):\n",
    "    \n",
    "    if (model[0] == 'fillmask'):\n",
    "        return \"TODO\" # TODO\n",
    "    elif (model[0] == 'translate' or model[0] == 'summarize' or model == 'q&a'):\n",
    "        output_original_input = request_to_model(model, original_input)\n",
    "        bing_output = await request_to_bing(original_input, model[0])\n",
    "        bard_output = request_to_bard(original_input, model[0])\n",
    "        output_original_input = next(iter(output_original_input[0].values()))\n",
    "        if (mode == 0):\n",
    "            similarity_bing_output = await get_distance_answers(output_original_input, bing_output, 0)\n",
    "            similarity_bard_output = await get_distance_answers(output_original_input, bard_output, 0)\n",
    "            return {\"bing\": similarity_bing_output > threshold, \"bard\": similarity_bard_output > threshold}\n",
    "        else:\n",
    "            similarity_other_bing = await get_distance_answers(output_original_input, bing_output, 1)\n",
    "            similarity_other_bard = await get_distance_answers(output_original_input, bard_output, 1)\n",
    "            return {\"bing\": similarity_other_bing > threshold, \"bard\": similarity_other_bard > threshold}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Response status code is not 200. Response Status is 404",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m distance_set_mrt(models[\u001b[38;5;241m9\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the capital of Spain?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m, in \u001b[0;36mdistance_set_mrt\u001b[1;34m(model, original_input, mode, threshold)\u001b[0m\n\u001b[0;32m      6\u001b[0m output_original_input \u001b[38;5;241m=\u001b[39m request_to_model(model, original_input)\n\u001b[0;32m      7\u001b[0m bing_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m request_to_bing(original_input, model[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m bard_output \u001b[38;5;241m=\u001b[39m \u001b[43mrequest_to_bard\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m output_original_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(output_original_input[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mrequest_to_bard\u001b[1;34m(question, question2, type)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompare\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     17\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompare the following sentences and give me only the \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m number of similarity, i dont want anyting else: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m bard \u001b[38;5;241m=\u001b[39m \u001b[43mBardCookies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcookie_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookie_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompare\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     21\u001b[0m     response \u001b[38;5;241m=\u001b[39m bard\u001b[38;5;241m.\u001b[39mget_answer(prompt \u001b[38;5;241m+\u001b[39m question \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m question2)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tcame\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bardapi\\core_cookies.py:53\u001b[0m, in \u001b[0;36mBardCookies.__init__\u001b[1;34m(self, cookie_dict, timeout, proxies, session, conversation_id, google_translator_api_key, language, run_code, token_from_browser)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoice_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_session(session)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSNlM0e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_snim0e\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage \u001b[38;5;241m=\u001b[39m language \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BARD_API_LANG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_code \u001b[38;5;241m=\u001b[39m run_code \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tcame\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bardapi\\core_cookies.py:114\u001b[0m, in \u001b[0;36mBardCookies._get_snim0e\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://bard.google.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, proxies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse status code is not 200. Response Status is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m snim0e \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNlM0e\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m(.*?)\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, resp\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m snim0e:\n",
      "\u001b[1;31mException\u001b[0m: Response status code is not 200. Response Status is 404"
     ]
    }
   ],
   "source": [
    "res = await distance_set_mrt(\"What is the capital of Spain?\", models[9])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
