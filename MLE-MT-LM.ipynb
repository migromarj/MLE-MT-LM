{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE - G9: Confiabilidad de Modelos de Lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nltk\n",
    "from functions.metamorphic import calculate_AFR, calculate_AFR_without_Bing\n",
    "from functions.perturbations import delete_characters, add_characters, add_random_words, remplace_named_entities, replace_characters, replace_words_with_antonyms, replace_words_with_synonyms, delete_sentences\n",
    "from functions.models import request_to_bing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "nltk.download('punkt')\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprobación de funcionamiento de Bing Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"BING_U_COOKIE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(await request_to_bing(\"What is the capital of Spain?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelos a evaluar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('summarize', 'facebook/bart-large-cnn'),\n",
    "    ('summarize', 'google/pegasus-large'),\n",
    "    ('toxic', 's-nlp/roberta_toxicity_classifier'),\n",
    "    ('toxic', 'citizenlab/distilbert-base-multilingual-cased-toxicity', 'inputs'),\n",
    "    ('spam', 'h-e-l-l-o/email-spam-classification-merged'),\n",
    "    ('spam', 'dima806/email-spam-detection-roberta'),\n",
    "    ('translate', 't5-base'),\n",
    "    ('translate', 'allenai/wmt16-en-de-12-1'),\n",
    "    ('fillmask', 'vinai/bertweet-base', '<mask>'),\n",
    "    ('fillmask', 'roberta-base', '<mask>'),\n",
    "]\n",
    "\n",
    "summarize_models = [models[0], models[1]]\n",
    "toxic_models = [models[2], models[3]]\n",
    "spam_models = [models[4], models[5]]\n",
    "translate_models = [models[6], models[7]]\n",
    "fillmask_models = [models[8], models[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluación de modelos sin hacer uso de Bing Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations_without_bing = [(delete_characters, False, 'delete_characters'), (replace_characters, False, 'replace_characters'), (add_characters, False, 'add_characters')]\n",
    "attributes_without_bing = [\"Robustness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def AFR_without_Bing(models, perturbations, attributes, iterations = 1):\n",
    "    RESULTS = pd.DataFrame(columns=[\"Value\", \"Time\"])\n",
    "    for model in models:\n",
    "        for perturbation in perturbations:\n",
    "            for attribute in attributes:\n",
    "                print(model[0], model[1], perturbation[2], attribute)\n",
    "                AFR, M = await calculate_AFR_without_Bing(model, perturbation[0], attribute, perturbation[1], iterations=iterations)\n",
    "                print(AFR, M)\n",
    "                RESULTS.loc[model[0] + \" - \" + model[1] + \" - \" + perturbation[2] + \" - \" + attribute] = [AFR, M]\n",
    "\n",
    "    return RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face models\n",
    "\n",
    "await AFR_without_Bing(summarize_models, perturbations_without_bing, attributes_without_bing, iterations=1)\n",
    "\n",
    "# Execute tests\n",
    "\n",
    "summarize_results = await AFR_without_Bing(summarize_models, perturbations_without_bing, attributes_without_bing, iterations=10)\n",
    "summarize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face models\n",
    "\n",
    "await AFR_without_Bing(toxic_models, perturbations_without_bing, attributes_without_bing, iterations=1)\n",
    "\n",
    "# Execute tests\n",
    "\n",
    "toxic_results = await AFR_without_Bing(toxic_models, perturbations_without_bing, attributes_without_bing, iterations=10)\n",
    "toxic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face models\n",
    "\n",
    "await AFR_without_Bing(spam_models, perturbations_without_bing, attributes_without_bing, iterations=1)\n",
    "\n",
    "# Execute tests\n",
    "\n",
    "spam_results = await AFR_without_Bing(spam_models, perturbations_without_bing, attributes_without_bing, iterations=10)\n",
    "spam_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face models\n",
    "\n",
    "await AFR_without_Bing(translate_models, perturbations_without_bing, attributes_without_bing, iterations=1)\n",
    "\n",
    "# Execute tests\n",
    "\n",
    "translate_results = await AFR_without_Bing(translate_models, perturbations_without_bing, attributes_without_bing, iterations=10)\n",
    "translate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face models\n",
    "\n",
    "await AFR_without_Bing(fillmask_models, perturbations_without_bing, attributes_without_bing, iterations=1)\n",
    "\n",
    "# Execute tests\n",
    "\n",
    "fill_masks_results = await AFR_without_Bing(fillmask_models, perturbations_without_bing, attributes_without_bing, iterations=10)\n",
    "fill_masks_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportación de resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results.to_csv('./results/results.csv', mode='a', header=False)\n",
    "toxic_results.to_csv('./results/results.csv', mode='a', header=False)\n",
    "spam_results.to_csv('./results/results.csv', mode='a', header=False)\n",
    "translate_results.to_csv('./results/results.csv', mode='a', header=False)\n",
    "fill_masks_results.to_csv('./results/results.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/results.csv', header=None, names=['Columna1', 'Score', 'Time'])\n",
    "\n",
    "df[['Task', 'Model', 'Perturbation', 'Attribute']] = df['Columna1'].str.split(' - ', expand=True)\n",
    "\n",
    "df['Score'] = pd.to_numeric(df['Score'])\n",
    "df['Time'] = pd.to_numeric(df['Time'])\n",
    "\n",
    "df = df.drop(columns=['Columna1'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score promedio por tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby('Task')['Score'].mean().plot(kind='bar', color='skyblue')\n",
    "plt.title('Score promedio por tarea')\n",
    "plt.xlabel('Tarea')\n",
    "plt.ylabel('Score promedio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiempo vs. Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(df['Time'], df['Score'], cmap='viridis', alpha=0.7)\n",
    "plt.title('Tiempo vs. Score')\n",
    "plt.xlabel('Tiempo (segundos)')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot para Comparar la Distribución de los Scores por Tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Task', y='Score', data=df, palette='viridis')\n",
    "plt.title('Distribución de Scores por Tarea')\n",
    "plt.xlabel('Tarea')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribución de Tiempos por Tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "df.groupby('Task')['Time'].hist(alpha=0.7, bins=20, stacked=True, legend=True, figsize=(12, 6))\n",
    "plt.title('Distribución de Tiempos por Tarea')\n",
    "plt.xlabel('Tiempo (segundos)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend(title='Tarea', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico de Violín para Distribución de Tiempo por Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Model', y='Time', data=df, palette='viridis')\n",
    "plt.title('Distribución de Tiempo por Modelo')\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Tiempo (segundos)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribución de Tiempo por Perturbación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Perturbation', y='Time', data=df, palette='viridis')\n",
    "plt.title('Distribución de Tiempo por Perturbación')\n",
    "plt.xlabel('Perturbación')\n",
    "plt.ylabel('Tiempo (segundos)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluación de modelos haciendo uso de Bing Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = [(delete_characters, False, 'delete_characters'), (replace_characters, False, 'replace_characters'), (add_characters, False, 'add_characters'),(replace_words_with_synonyms, False, 'replace_word_synonyms'), (replace_words_with_antonyms, True,'replace_word_antonyms'), (add_random_words, False,'add_random_words'), (remplace_named_entities, False,'remplace_named_entities')]\n",
    "attributes = [\"Robustness\", \"Non-determinism\", \"Fairness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def AFR(models, perturbations, attributes):\n",
    "    RESULTS = pd.DataFrame(columns=[\"Value\", \"Time\"])\n",
    "    for model in models:\n",
    "        m_type = model[0]\n",
    "        for perturbation in perturbations:\n",
    "            new_attributes = attributes.copy() if m_type != \"summarize\" else attributes.copy()[:-1]\n",
    "            for attribute in new_attributes:\n",
    "                print(model[0], model[1], perturbation[2], attribute)\n",
    "                AFR, t = await calculate_AFR(model, perturbation[0], attribute, perturbation[2], perturbation[1], iterations=1)\n",
    "                print(AFR, t)\n",
    "                RESULTS.loc[model[0] + \" - \" + model[1] + \" - \" + perturbation[2] + \" - \" + attribute] = [AFR, t]\n",
    "\n",
    "    return RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_results = await AFR(summarize_models, perturbations, attributes)\n",
    "summarize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_results = await AFR(toxic_models, perturbations, attributes)\n",
    "toxic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_results = await AFR(spam_models, perturbations, attributes)\n",
    "spam_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_results = await AFR(translate_models, perturbations, attributes)\n",
    "translate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_masks_results = await AFR(fillmask_models, perturbations, attributes)\n",
    "fill_masks_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
